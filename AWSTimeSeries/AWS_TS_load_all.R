#### Get the City Data ####
# Load the helper functions
source("Helpers/Postgres_functions.R")
source("Helpers/SpatialUtil.R")

# Connect To the Database
con <- connectDB(db)

# Pull in the data for a given city, including past the 1-27-17 cutoff.
loc.data = dbGetQuery(con, paste0("
                      WITH casted AS(
	SELECT id, CAST(created_at as timestamptz) as created_at, source, text_lang, user_id,
                      user_lang
                      FROM ",db,"_city_primary
                      WHERE source ILIKE ANY(ARRAY['%for Blackberry%', '%for Android%', '%tron%',
                       '%Foursquare%','%Instagram%','%for iOS%',
                                  '%for iPhone%','%for Windows Phone%',
                                  '%for iPad%','Twitter Web Client','%for Mac%'])
)

SELECT timezone('",tz,"',created_at) as tzone,
text_lang, user_lang, user_id, source
           FROM casted
           WHERE timezone('",tz,"',created_at) >= '2016-10-28' ;"))

# Disconnect
disconnectDB(con)


#### Merging and Cleaning ####
library(dplyr)
library(lubridate)
library(tidyr)

# Load the Twitter language registry
registry <- load_langs()

# Merge the language by the users' selected language!
data_merge <- left_join(loc.data, registry)
data_merge$language <- as.factor(data_merge$language)

# Clean up the source (remove hyperlinks), Extract the Date
data_merge <- data_merge %>%
  mutate(source = gsub(pattern = "<.+\">|</a>", "",source),
         date = format(tzone, "%d %b %Y"))

# Merge Topics
clean.topics <- merge_topics(data_merge, "Topic_Data/")

# Remove duplicate files
rm(loc.data, data_merge)

# Aggregate count by date and topic
counts <- clean.topics %>%
  # Extract Date
  mutate(date = as.Date(date,"%d %b %Y")) %>%
  # Filter out languages in the registry, so it's only the topics
  # filter(!(lang.topic %in% registry$language)) %>%
  # Filter out NA values
  filter(!is.na(lang.topic)) %>%
  # Provide the count by day for each topic group.
  group_by(date, lang.topic) %>% count

# Spread the data from long to wide format and sort by date
counts <- counts %>%
  spread(lang.topic,n) %>%
  arrange(date)

# Turn NAs to 0
counts[is.na(counts)] <- 0

# Top topics, by count
top_topics <- clean.topics %>%
  group_by(lang.topic) %>%
  filter(!is.na(lang.topic) & date <= "2017-01-27") %>%
  count() %>%
  arrange(desc(n))

#### Smoothing and ARIMA modeling ####
library(forecast)

# Apply 7-day rolling average to the counts
counts_filtered <- apply(counts[,-1],2, stats::filter, rep(1/7,7))

# Rebind to dates
counts_filtered <- cbind(counts$date, as.data.frame(counts_filtered))

# Use forcast's auto.arima to estimate models based upon a user-selected criterion:
# can be aicc, aic, or bic.
arimaTrace <- function(data, ic = "aicc"){
  require(forecast)
  require(dplyr)
  # Capture the output of auto.arima, which runs in parallel and outputs a trace.
  out <- capture.output({
    fit <- auto.arima(data,ic=ic,trace=T,stepwise = F, parallel = T)
  })
  # Add the trace to the fit, in case it's needed later.
  fit$trace <- read.table(t <- textConnection(out), sep=":", col.names = c("model",ic)) # %>% arrange(AIC)
  # Close the text connection.
  close(t)
  # Return fit
  fit
}

# Apply to all the columns, except the date
models <- apply(counts[,-1], 2, arimaTrace)

# Apply to the smoothed version
smooth.models <- apply(counts_filtered[,-1], 2, arimaTrace)

# A function to extract the coefficients from the models generated by arimaTrace,
# along with ARIMA values
extract_coefs <- function(models){
  coefs <- as.data.frame(matrix(NA, nrow = length(names(models)), ncol = 12))
  names(coefs) <- c("topic", "p","d","q","ar1","ar2","ar3","ar4","ma1","ma2","ma3","ma4")
  coefs$topic <- names(models)
  for (i in 1:length(models)){
    coefs$ar1[i] <- models[[i]][[1]]["ar1"]
    coefs$ar2[i] <- models[[i]][[1]]["ar2"]
    coefs$ar3[i] <- models[[i]][[1]]["ar3"]
    coefs$ar4[i] <- models[[i]][[1]]["ar4"]
    coefs$ma1[i] <- models[[i]][[1]]["ma1"]
    coefs$ma2[i] <- models[[i]][[1]]["ma2"]
    coefs$ma3[i] <- models[[i]][[1]]["ma3"]
    coefs$ma4[i] <- models[[i]][[1]]["ma4"]
    coefs$p[i] <- models[[i]][["arma"]][1]
    coefs$d[i] <- models[[i]][["arma"]][6]
    coefs$q[i] <- models[[i]][["arma"]][2]
  }
  coefs
}

3 Extract the coefficients for the smoothed and unsmoothed topic-series.
coefs <- extract_coefs(models)
smooth.coefs <- extract_coefs(smooth.models)

# find the number of unique users by language
unique.users <- clean.topics %>%
  group_by(lang.topic) %>%
  filter(!is.na(lang.topic)) %>%
  mutate(users = length(unique(user_id))) %>%
  select(lang.topic, users) %>%
  unique %>%
  arrange(desc(users))

# Top topics, by count
top_topics <- clean.topics %>%
  group_by(lang.topic) %>%
  filter(!is.na(lang.topic)) %>%
  count() %>%
  arrange(desc(n))

# Merge the number of unique users, Tweets per User and coefficients for smoothed
# and unsmoothed time series
coefs2 <- top_topics[1:20,] %>%
  rename(tweets = n) %>%
  left_join(unique.users, by ="lang.topic") %>%
  mutate (avg = (tweets/users)) %>%
  left_join(coefs, by = c("lang.topic"="topic"))

smooth.coefs2 <- top_topics[1:20,] %>%
  rename(tweets = n) %>%
  left_join(unique.users, by ="lang.topic") %>%
  mutate (avg = (tweets/users)) %>%
  left_join(smooth.coefs, by = c("lang.topic"="topic"))

# Write the outputs to CSV
write.csv(coefs2,paste0("Outputs/",db,"_model_coefficients.csv"))
write.csv(smooth.coefs2,paste0("Outputs/",db,"_smooth_model_coefficients.csv"))


#### Correlation Clustering ####
library(ggplot2)
library(forecast)

# Summarize the count by topic, by day
count.TopicDay <- clean.topics %>%
  mutate(day = wday(tzone, label = T)) %>%
  group_by(lang.topic, day, time = as.Date(tzone)) %>%
  tally() %>%
  ungroup %>%
  group_by(lang.topic, day) %>%
  summarise(avg = mean(n))

# Sample cross-correlation plot; 0 is significant, so just use straight correlation
# autoplot(Ccf(counts$`Topic 0 - English`, counts$`Topic 1 - English`))

# Initialize a Data Frame to hold the correlations
corrs <- as.data.frame(matrix(0, nrow = ncol(counts)-1, ncol(counts)))
names(corrs) <- c("topic",names(counts)[-1])

# Insert the correlations (i.e. lag 0) into the above
for (i in 2:ncol(counts)){
  corrs[i-1,1] <- names(counts)[i]
  for (j in 2:ncol(counts)){
    corrs[i-1,j] <- cor(counts[[i]],counts[[j]])
  }
}

# Insert 0's for NA values
corrs[is.na(corrs)] <- 0

# Turn correlation into a dissimilarity metric by subtracting it from 1.
dissim <- 1-corrs[,-1]

# Create clusters using the correlations as the distance metric
clusters <- hclust(as.dist(dissim))
plot(clusters,  labels = corrs$topic)

# Create five clusters, write them to a CSV
clust <- cutree(clusters, k =5)
write.csv(clust,paste0("Outputs/",db,"AllTopics_5Clusters.csv"))

# Plot dendrogram of all Topics
library(sparcl)
# colors the leaves of a dendrogram
png(paste0("Outputs/",db,"-AllTopicsDendrogram.png"),width = 11, height = 8.5, units = "in", res = 300)
ColorDendrogram(clusters, y = clust, labels = names(clust),
                main = paste("Dendrogram of All Topics in",db),
                xlab = "Topics", sub = "",
                branchlength = .2)
dev.off()

# Repeat the above, but normalized per user.
# Normalizing tweets per user
tweetcount <- clean.topics %>%
  mutate(date = as.Date(date,"%d %b %Y")) %>%
  #filter(!(lang.topic %in% registry$language)) %>%
  filter(!is.na(lang.topic)) %>%
  group_by(date, lang.topic) %>% count

activityPerUser <- left_join(tweetcount,unique.users, by ="lang.topic") %>%
  rename(tweets = n ) %>%
  mutate(PerUser = tweets/users) %>%
  select(date, lang.topic, PerUser)

activityPerUser <- activityPerUser %>%
  spread(lang.topic,PerUser) %>%
  arrange(date)

# Initialize the Data Frame
corrs.norm <- as.data.frame(matrix(0, nrow = ncol(activityPerUser)-1, ncol(activityPerUser)))
names(corrs.norm) <- c("topic",names(activityPerUser)[-1])

# Insert the correlations (i.e. lag 0)
for (i in 2:ncol(activityPerUser)){
  corrs.norm[i-1,1] <- names(activityPerUser)[i]
  for (j in 2:ncol(activityPerUser)){
    corrs.norm[i-1,j] <- cor(activityPerUser[[i]],activityPerUser[[j]])
  }
}

corrs.norm[is.na(corrs.norm)] <- 0

dissim <- 1-corrs.norm[,-1]
# Create clusters using the correlations as the distance metric
clusters2 <- hclust(as.dist(dissim))
plot(clusters2,  labels = corrs.norm$topic)

# Five clusters
norm.clust <- cutree(clusters2, k =5)

png(paste0("Outputs/",db,"-AllTopicsNormDendrogram.png"),width = 11, height = 8.5, units = "in", res = 300)
# colors the leaves of a dendrogram
ColorDendrogram(clusters2, y = norm.clust, labels = names(norm.clust),
                main = paste("Dendrogram of All Topics in",db),
                xlab = "Topics", sub = "Normalized by Tweets per User",
                branchlength = .2)
dev.off()

#### Dendrograms for top 20 Topics  ####
# Follows the same process above, albeit for the top 20 topics.

tweetcount <- clean.topics %>%
  mutate(date = as.Date(date,"%d %b %Y")) %>%
  filter(lang.topic %in% top_topics$lang.topic[1:20]) %>%
  group_by(date, lang.topic) %>% count

activityPerUser <- left_join(tweetcount,unique.users, by ="lang.topic") %>%
  rename(tweets = n ) %>%
  mutate(PerUser = tweets/users) %>%
  select(date, lang.topic, PerUser)

activityPerUser <- activityPerUser %>%
  spread(lang.topic,PerUser) %>%
  arrange(date)

tweetcount <- tweetcount %>%
  spread(lang.topic,n) %>%
  arrange(date)

# Initialize a Data Frame to hold the correlations
corrs <- as.data.frame(matrix(0, nrow = ncol(tweetcount)-1, ncol(tweetcount)))
names(corrs) <- c("topic",names(tweetcount)[-1])

# Insert the correlations (i.e. lag 0) into the above
for (i in 2:ncol(tweetcount)){
  corrs[i-1,1] <- names(tweetcount)[i]
  for (j in 2:ncol(tweetcount)){
    corrs[i-1,j] <- cor(tweetcount[[i]],tweetcount[[j]])
  }
}

# Insert 0's for NA values
corrs[is.na(corrs)] <- 0

dissim <- 1-corrs[,-1]
# Create clusters using the correlations as the distance metric
clusters <- hclust(as.dist(dissim))
plot(clusters,  labels = corrs$topic)

# Five clusters
clust <- cutree(clusters, k =5)
write.csv(clust,paste0("Outputs/",db,"Top20Topics_5Clusters.csv"))


library(sparcl)
# colors the leaves of a dendrogram
png(paste0("Outputs/",db,"-Top20TopicsDendrogram.png"),width = 11, height = 8.5, units = "in", res = 300)
ColorDendrogram(clusters, y = clust, labels = names(clust),
                main = paste("Dendrogram of Top 20 Topics in",db),
                xlab = "Topics", sub = "",
                branchlength = .2)
dev.off()
# Initialize the Data Frame
corrs.norm <- as.data.frame(matrix(0, nrow = ncol(activityPerUser)-1, ncol(activityPerUser)))
names(corrs.norm) <- c("topic",names(activityPerUser)[-1])

# Insert the correlations (i.e. lag 0)
for (i in 2:ncol(activityPerUser)){
  corrs.norm[i-1,1] <- names(activityPerUser)[i]
  for (j in 2:ncol(activityPerUser)){
    corrs.norm[i-1,j] <- cor(activityPerUser[[i]],activityPerUser[[j]])
  }
}

corrs.norm[is.na(corrs.norm)] <- 0

dissim <- 1-corrs.norm[,-1]
# Create clusters using the correlations as the distance metric
clusters2 <- hclust(as.dist(dissim))
plot(clusters2,  labels = corrs.norm$topic)

# Five clusters
norm.clust <- cutree(clusters2, k =5)

png(paste0("Outputs/",db,"-Top20TopicsNormDendrogram.png"),width = 11, height = 8.5, units = "in", res = 300)
# colors the leaves of a dendrogram
ColorDendrogram(clusters2, y = norm.clust, labels = names(norm.clust),
                main = paste("Dendrogram of Top 20 Topics in",db),
                xlab = "Topics", sub = "Normalized by Tweets per User",
                branchlength = .2)
dev.off()

#### Forecasting ####
library(forecast)

# find the number of unique users by language
unique.users <- clean.topics %>%
  group_by(lang.topic) %>%
  filter(!is.na(lang.topic)) %>%
  mutate(users = length(unique(user_id))) %>%
  select(lang.topic, users) %>%
  unique %>%
  arrange(desc(users))

# Gather the languages
tweetcount <- counts %>% gather(lang.topic,n, -date)

# Normalize the counts per user and spread the data frame
activityPerUser <- left_join(tweetcount,unique.users, by ="lang.topic") %>%
  rename(tweets = n ) %>%
  mutate(PerUser = tweets/users) %>%
  select(date, lang.topic, PerUser)

activityPerUser <- activityPerUser %>%
  spread(lang.topic,PerUser) %>%
  arrange(date)

activityPerUser[is.na(activityPerUser)] <- 0


# Read the coefficients from CSV
coefs <- read.csv(paste0("Outputs/",db,"_model_coefficients.csv"))

# Split Tweets per User into training and testing sets
# Training will be the study period
train <- activityPerUser %>%
  filter(date < "2017-01-28") %>%
  select(date, one_of(
    as.vector(coefs$lang.topic)))

# Testing will be all subsequent activity.
test <- activityPerUser %>%
  filter(date >= "2017-01-28") %>%
  select(date, one_of(
    as.vector(coefs$lang.topic)))

# Function to fit a model with the parameters specified:
# x is the univariate time series, h is the number of predictions,
# p,d, and q are standard ARIMA parameters, with s being the period
# for seasonality, if applicable.

fit <- function(x,h,p,d,q,P=0,D=0,Q=0,s=1){
  forecast(Arima(x, order = c(p,d,q), seasonal = c(P,D,Q,s)), h=h)
}

# Prepare a data frame to hold the results for predictions from 1 to 7 days ahead.
CVmodels <- as.data.frame(matrix(nrow = ncol(train)-1, ncol=8))
names(CVmodels) <- c("lang.topic","1day", "2day","3day","4day","5day","6day","7day")

# Add the topic names
CVmodels$lang.topic <- names(train[2:ncol(train)])

# Loop over the topics, using tsCV from Forescast, whchc uses a rolling window.
for(topic.i in names(train)[2:ncol(train)]){
  params <- coefs %>%
    select(lang.topic, p,d,q) %>%
    filter (lang.topic == topic.i)
  for (i in 1:7){
    CVresults <- tsCV(train[,topic.i], fit, h=i, p = params$p, d = params$d, q= params$q)
    CVmodels[CVmodels$lang.topic == topic.i,i+1] <- sum(CVresults[!is.na(CVresults)]^2)/sum(!is.na(CVresults))
  }
}

# Mean normalized counts
temp <- as.data.frame(round(sapply(train[2:ncol(train)],mean),4)) %>%
  mutate(lang.topic = rownames(.))
names(temp)[1] <- "avg"

# Write to a CSV
MSEoutput <- left_join(CVmodels,temp)
write.csv(MSEoutput,paste0("Outputs/",db,"_CV_MSE.csv"))

#### Plotting Series ####
library(ggplot2)

plotting <- CVmodels %>% gather(window,perUser,-lang.topic) %>% group_by(lang.topic)

ggplot(plotting %>% filter(lang.topic %in% coefs$lang.topic[1:20]), aes(x=window, y=perUser)) +
  geom_line(aes(group = lang.topic, color=lang.topic)) +
  scale_color_discrete() +
  ggtitle(paste("Crossvalidated MSE for Top 20 topics in",db)) +
  ggsave(paste0("Outputs/",db,"-CV_MSE.png"), width = 11, height = 8.5, units = "in")
