labs(y = "Tweets (100,000)") +
scale_fill_manual(values = rev(brewer.pal(3,"Set1")),name ="",
breaks = c("Instagram", "Foursquare","Other")) +
theme(legend.position = "top", legend.box.margin = margin(0,0,0,0),
axis.title.x = element_blank(),
plot.margin = margin(0,10,10,10),
axis.text.x = element_text(size = 21))+
ggsave(paste0("Outputs/NewSources.png"), width = 9.1198, height = 8.156, units = "in")
ggplot(counts,
aes(city)) +
geom_bar(stat="identity", aes(y = value/100000,
fill= source)) +
labs(y = "Tweets (100,000)") +
scale_fill_manual(values = rev(brewer.pal(3,"Set1")),name ="",
breaks = c("Instagram", "Foursquare","Other")) +
theme(legend.position = "none", legend.box.margin = margin(0,0,0,0),
axis.title.x = element_blank(),
plot.margin = margin(0,10,10,10),
axis.text.x = element_text(size = 21))+
ggsave(paste0("Outputs/NewSources_nolegend.png"), width = 9.1198, height = 8.156, units = "in")
ggplot(counts,
aes(city)) +
geom_bar(stat="identity", aes(y = value/100000,
fill= source)) +
labs(y = "Tweets (100,000)") +
scale_fill_manual(values = rev(brewer.pal(3,"Set1")),name ="",
breaks = c("Instagram", "Foursquare","Other")) +
theme(legend.position = "bottom", legend.box.margin = margin(0,0,0,0),
axis.title.x = element_blank(),
plot.margin = margin(0,10,10,10),
axis.text.x = element_text(size = 21))+
ggsave(paste0("Outputs/NewSources.png"), width = 9.1198, height = 8.156, units = "in")
con <- connectDB(db)
con <- connectDB(tolower(db))
con <- connectDB(db)
)
con <- connectDB(tolower(db))
LA.all <- dbGetQuery(con,
"SELECT *
FROM geometries_filter")
disconnectDB(con)
source("IST_meta.R")
con <- connectDB(tolower(db))
IST.all <- dbGetQuery(con,
"SELECT *
FROM geometries_filter")
disconnectDB(con)
LAX.map <- ggmap(get_map(location = bbox, source = "stamen", maptype="toner", color = "bw")) +
theme(axis.text.x = element_blank(),
axis.title.x = element_blank(),
axis.text.y = element_blank(),
axis.title.y = element_blank(),
axis.ticks = element_blank())
library(ggmap)
source("LAX_meta.R")
LAX.map <- ggmap(get_map(location = bbox, source = "stamen", maptype="toner", color = "bw")) +
theme(axis.text.x = element_blank(),
axis.title.x = element_blank(),
axis.text.y = element_blank(),
axis.title.y = element_blank(),
axis.ticks = element_blank())
map <- LAX.map
library(dplyr)
library(readr)
LA.clean <- merge_langs(LA.all)
load_langs <- function(){
require(jsonlite)
registry <- fromJSON('../Assets/Langauge.json')
names(registry) <- c('user_lang', 'language')
registry[1:2]
}
LA.clean <- merge_langs(LA.all)
registry <- fromJSON('../Assets/Langauge.json')
registry <- fromJSON("../Assets/Langauge.json")
registry <- fromJSON("/Assets/Langauge.json")
?fromJSON
fromJSON(file = "../Assets/Language.json")
library(jsonlite)
fromJSON(Language.json)
fromJSON("Language.json")
?fromJSON
registry <- jsonlite::fromJSON("../Assets/Langauge.json")
registry <- rjson::fromJSON(file"../Assets/Langauge.json")
registry <- rjson::fromJSON(file="../Assets/Langauge.json")
registry <- rjson::fromJSON(file="../Assets/Langauge.json")
registry <- fromJSON(file="../Assets/Langauge.json")
registry <- jsonlite::fromJSON(file="../Assets/Langauge.json")
registry <- jsonlite::fromJSON("../Assets/Langauge.json")
registry <- fromJSON('../../Cultural_Mapper/Assets/Langauge.json')
rm(registry)
load_langs <- function(){
require(jsonlite)
registry <- fromJSON('../../Cultural_Mapper/Assets/Langauge.json')
names(registry) <- c('user_lang', 'language')
registry[1:2]
}
LA.clean <- merge_langs(LA.all)
clean.topics <- merge_topics(clean, "Topic_Data/")
clean.topics <- merge_topics(LA.clean, "Topic_Data/")
top_langs <- clean.topics %>% group_by(lang.topic) %>%
count() %>%
mutate(percent = round(100*n/nrow(clean),2)) %>%
arrange(desc(percent))
top_langs <- clean.topics %>% group_by(lang.topic) %>%
count() %>%
mutate(percent = round(100*n/nrow(clean.topics),2)) %>%
arrange(desc(percent))
top_langs
map + scale_color_brewer(palette = "Set1", name ="Topics") +
geom_density2d(aes(x = long, y = lat, color = lang.topic), alpha = .5,
n = grid.points,
data = clean.topics %>% filter(source != "Instagram",
lang.topic %in% top_langs$lang.topic[1:6])) +
facet_grid(day ~ hour.cut) +
ggtitle(paste("Densities of Top 6 Topics in",db,"by Day by Hour"))  +
theme(legend.position = "bottom", legend.box = "horizontal")
grid.points <- findN(map,200,epsg)
source("SpatialUtil.R")
meters <- LongLatToM(clean.topics$long,clean.topics$lat,epsg)
topic_meters <- cbind(clean.topics,meters)
grid.points <- findN(map,200,epsg)
map + scale_color_brewer(palette = "Set1", name ="Topics") +
geom_density2d(aes(x = long, y = lat, color = lang.topic), alpha = .5,
n = grid.points,
data = clean.topics %>% filter(source != "Instagram",
lang.topic %in% top_langs$lang.topic[1:6])) +
facet_grid(day ~ hour.cut) +
ggtitle(paste("Densities of Top 6 Topics in",db,"by Day by Hour"))  +
theme(legend.position = "bottom", legend.box = "horizontal")
map + scale_color_brewer(palette = "Set1", name ="Topics") +
geom_density2d(aes(x = long, y = lat, color = lang.topic), alpha = .5,
n = grid.points,
data = clean.topics %>% filter(source != "Instagram",
lang.topic %in% top_langs$lang.topic[6])) +
facet_grid(day ~ hour.cut) +
theme(legend.position = "bottom", legend.box = "horizontal",
legend.box.margin = margin(0,0,0,0),
axis.title.x = element_blank(),
plot.margin = margin(0,10,10,10))
map + scale_color_brewer(palette = "Set1", name ="Topics") +
geom_density2d(aes(x = long, y = lat, color = lang.topic), alpha = .5,
n = grid.points,
data = clean.topics %>% filter(source != "Instagram",
lang.topic %in% top_langs$lang.topic[1:6])) +
facet_grid(day ~ hour.cut) +
theme(legend.position = "bottom", legend.box = "horizontal",
legend.box.margin = margin(0,0,0,0),
plot.margin = margin(0,10,10,10))+
ggsave(paste0("Outputs/",db,"-TopicDensity_DayHour_NEW.png"), width = 11, height = 8.5, units = "in")
map + scale_color_brewer(palette = "Set1", name ="Topics") +
geom_density2d(aes(x = long, y = lat, color = lang.topic), alpha = .5,
n = grid.points,
data = clean.topics %>% filter(source != "Instagram",
lang.topic %in% top_langs$lang.topic[1:6])) +
facet_grid(day ~ hour.cut) +
theme(legend.position = "bottom", legend.box = "horizontal",
legend.box.margin = margin(0,0,0,0),
plot.margin = margin(0,10,10,10))+
ggsave(paste0("Outputs/",db,"-TopicDensity_DayHour_NEW.png"), width = 6.4808, height = 8.0727, units = "in")
IST.clean <- merge_langs(IST.all)
clean.topics <- merge_topics(IST.clean, "Topic_Data/")
all_langs <- clean.topics %>%
group_by(language) %>%
count() %>%
mutate(percent = round(100*n/nrow(clean),2)) %>%
arrange(desc(n))
source("SpatialUtil.R")
meters <- LongLatToM(clean.topics$long,clean.topics$lat,epsg)
topic_meters <- cbind(clean.topics,meters)
IST.map <- ggmap(get_map(location = bbox, source = "stamen", maptype="toner", color = "bw")) +
theme(axis.text.x = element_blank(),
axis.title.x = element_blank(),
axis.text.y = element_blank(),
axis.title.y = element_blank(),
axis.ticks = element_blank())
map <- IST.map
map +
geom_point(aes(x = long, y = lat, color = lang.topic),
data = clean.topics %>% filter(user_language == all_langs$language[1],
source != "Instagram",
hour.cut == "8pm-12am",
tz >= "2016-12-31",
tz <= "2017-01-01"
),  alpha = .1) +
stat_density2d(aes(x = long, y = lat,
fill = ..level..), geom = "polygon",
n = grid.points,
data = clean.topics %>% filter(user_language == all_langs$language[1],
source != "Instagram",
hour.cut == "8pm-12am",
tz >= "2016-12-31",
tz <= "2017-01-01"
)) + theme(legend.position = "none") +
scale_fill_distiller(palette = "Spectral")
map +
geom_point(aes(x = long, y = lat, color = lang.topic), alpha = .1,
data = clean.topics %>% filter(user_language == all_langs$language[1:6],
source != "Instagram",
hour.cut == "8pm-12am",
tz != strftime("2016-01-01"),
day == "Sun"
)) +
stat_density2d(aes(x = long, y = lat,
fill = ..level..), geom = "polygon",
n = grid.points,
data = clean.topics %>% filter(user_language == all_langs$language[1:6],
source != "Instagram",
hour.cut == "8pm-12am",
tz != strftime("2017-01-01"),
day == "Sun"
))+
scale_fill_distiller(palette = "Spectral") +
theme(legend.position = "none")
all_langs <- clean.topics %>%
group_by(language) %>%
count() %>%
mutate(percent = round(100*n/nrow(clean),2)) %>%
arrange(desc(n))
all_langs <- clean.topics %>%
group_by(language) %>%
count() %>%
mutate(percent = round(100*n/nrow(clean.topics),2)) %>%
arrange(desc(n))
map <- IST.map
map +
geom_point(aes(x = long, y = lat, color = lang.topic),
data = clean.topics %>% filter(user_language == all_langs$language[1],
source != "Instagram",
hour.cut == "8pm-12am",
tz >= "2016-12-31",
tz <= "2017-01-01"
),  alpha = .1) +
stat_density2d(aes(x = long, y = lat,
fill = ..level..), geom = "polygon",
n = grid.points,
data = clean.topics %>% filter(user_language == all_langs$language[1],
source != "Instagram",
hour.cut == "8pm-12am",
tz >= "2016-12-31",
tz <= "2017-01-01"
)) + theme(legend.position = "none") +
scale_fill_distiller(palette = "Spectral")
source("IST_meta.R")
IST.clean <- merge_langs(IST.all)
clean.topics <- merge_topics(IST.clean, "Topic_Data/")
all_langs <- clean.topics %>%
group_by(language) %>%
count() %>%
mutate(percent = round(100*n/nrow(clean.topics),2)) %>%
arrange(desc(n))
source("SpatialUtil.R")
meters <- LongLatToM(clean.topics$long,clean.topics$lat,epsg)
topic_meters <- cbind(clean.topics,meters)
IST.map <- ggmap(get_map(location = bbox, source = "stamen", maptype="toner", color = "bw")) +
theme(axis.text.x = element_blank(),
axis.title.x = element_blank(),
axis.text.y = element_blank(),
axis.title.y = element_blank(),
axis.ticks = element_blank())
map <- IST.map
map
map +
geom_point(aes(x = long, y = lat, color = lang.topic),
data = clean.topics %>% filter(user_language == all_langs$language[1],
source != "Instagram",
hour.cut == "8pm-12am",
tz >= "2016-12-31",
tz <= "2017-01-01"
),  alpha = .1) +
stat_density2d(aes(x = long, y = lat,
fill = ..level..), geom = "polygon",
n = grid.points,
data = clean.topics %>% filter(user_language == all_langs$language[1],
source != "Instagram",
hour.cut == "8pm-12am",
tz >= "2016-12-31",
tz <= "2017-01-01"
)) + theme(legend.position = "none") +
scale_fill_distiller(palette = "Spectral")
map +
geom_point(aes(x = long, y = lat, color = lang.topic),
data = clean.topics %>% filter(user_language == all_langs$language[1],
source != "Instagram",
hour.cut == "8pm-12am",
tz >= "2016-12-31",
tz <= "2017-01-01"
),  alpha = .1) +
stat_density2d(aes(x = long, y = lat,
fill = ..level..), geom = "polygon",
n = grid.points,
data = clean.topics %>% filter(user_language == all_langs$language[1],
source != "Instagram",
hour.cut == "8pm-12am",
tz >= "2016-12-31",
tz <= "2017-01-01"
)) + theme(legend.position = "none") +
scale_fill_distiller(palette = "Spectral") +
ggsave(paste0("Outputs/",db,"-TopLangNYE_NEW.png"), width = 6.7, height = 4, units = "in")
map +
geom_point(aes(x = long, y = lat, color = lang.topic), alpha = .1,
data = clean.topics %>% filter(user_language == all_langs$language[1:6],
source != "Instagram",
hour.cut == "8pm-12am",
tz != strftime("2016-01-01"),
day == "Sun"
)) +
stat_density2d(aes(x = long, y = lat,
fill = ..level..), geom = "polygon",
n = grid.points,
data = clean.topics %>% filter(user_language == all_langs$language[1:6],
source != "Instagram",
hour.cut == "8pm-12am",
tz != strftime("2017-01-01"),
day == "Sun"
))+
scale_fill_distiller(palette = "Spectral") +
theme(legend.position = "none")
map +
geom_point(aes(x = long, y = lat, color = lang.topic), alpha = .1,
data = clean.topics %>% filter(user_language == all_langs$language[1:6],
source != "Instagram",
hour.cut == "8pm-12am",
tz != strftime("2016-01-01"),
day == "Sun"
)) +
stat_density2d(aes(x = long, y = lat,
fill = ..level..), geom = "polygon",
n = grid.points,
data = clean.topics %>% filter(user_language == all_langs$language[1:6],
source != "Instagram",
hour.cut == "8pm-12am",
tz != strftime("2017-01-01"),
day == "Sun"
))+
scale_fill_distiller(palette = "Spectral") +
theme(legend.position = "none") +
ggsave(paste0("Outputs/",db,"-TopLangSunNoNYE_NEW.png"), width = 6.7, height = 4, units = "in")
map +
geom_point(aes(x = long, y = lat, color = lang.topic), alpha = .1,
data = clean.topics %>% filter(user_language == all_langs$language[1],
source != "Instagram",
hour.cut == "8pm-12am",
tz != strftime("2016-01-01"),
day == "Sun"
)) +
stat_density2d(aes(x = long, y = lat,
fill = ..level..), geom = "polygon",
n = grid.points,
data = clean.topics %>% filter(user_language == all_langs$language[1],
source != "Instagram",
hour.cut == "8pm-12am",
tz != strftime("2017-01-01"),
day == "Sun"
))+
scale_fill_distiller(palette = "Spectral") +
theme(legend.position = "none") +
ggsave(paste0("Outputs/",db,"-TopLangSunNoNYE_NEW.png"), width = 6.7, height = 4, units = "in")
map +
geom_point(aes(x = long, y = lat, color = lang.topic),
data = clean.topics %>% filter(user_language == all_langs$language[1],
source != "Instagram",
hour.cut == "12am-4am",
tz >= "2016-12-31",
tz <= "2017-01-01"
),  alpha = .1) +
stat_density2d(aes(x = long, y = lat,
fill = ..level..), geom = "polygon",
n = grid.points,
data = clean.topics %>% filter(user_language == all_langs$language[1],
source != "Instagram",
hour.cut == "12am-4am",
tz >= "2016-12-31",
tz <= "2017-01-01"
)) + theme(legend.position = "none") +
scale_fill_distiller(palette = "Spectral")
unique(clean.topics$hour.cut)
clean.topics %>% filter(user_language == all_langs$language[1],
source != "Instagram",
hour.cut == "12am-4am",
tz >= "2016-12-31",
tz <= "2017-01-01"
)
map +
geom_point(aes(x = long, y = lat, color = lang.topic),
data = clean.topics %>% filter(user_language == all_langs$language[1],
source != "Instagram",
hour.cut == "12am-4am",
tz >= "2016-12-31",
tz <= "2017-01-01"
),  alpha = .1) +
stat_density2d(aes(x = long, y = lat,
fill = ..level..), geom = "polygon",
n = grid.points,
data = clean.topics %>% filter(user_language == all_langs$language[1],
source != "Instagram",
hour.cut == "12am-4am",
tz >= "2016-12-31",
tz <= "2017-01-01"
)) + theme(legend.position = "none") +
scale_fill_distiller(palette = "Spectral")
grid.points <- findN(map,200,epsg)
map +
geom_point(aes(x = long, y = lat, color = lang.topic),
data = clean.topics %>% filter(user_language == all_langs$language[1],
source != "Instagram",
hour.cut == "12am-4am",
tz >= "2016-12-31",
tz <= "2017-01-01"
),  alpha = .1) +
stat_density2d(aes(x = long, y = lat,
fill = ..level..), geom = "polygon",
n = grid.points,
data = clean.topics %>% filter(user_language == all_langs$language[1],
source != "Instagram",
hour.cut == "12am-4am",
tz >= "2016-12-31",
tz <= "2017-01-01"
)) + theme(legend.position = "none") +
scale_fill_distiller(palette = "Spectral")
map +
stat_density2d(aes(x = long, y = lat,
fill = ..level..), geom = "polygon",
n = grid.points,
data = clean.topics %>% filter(user_language == all_langs$language[1],
source != "Instagram",
hour.cut == "12am-4am",
tz >= "2016-12-31",
tz <= "2017-01-01"
)) + theme(legend.position = "none") +
scale_fill_distiller(palette = "Spectral")
map
map +
geom_point(aes(x = long, y = lat, color = lang.topic), alpha = .1,
data = clean.topics %>% filter(user_language == all_langs$language[1],
source != "Instagram",
hour.cut == "12am-4am",
tz != strftime("2016-01-01"),
day == "Sun"
))
map + stat_density2d(aes(x = long, y = lat,
fill = ..level..), geom = "polygon",
n = grid.points,
data = clean.topics %>% filter(user_language == all_langs$language[1],
source != "Instagram",
hour.cut == "12am-4am",
tz != strftime("2017-01-01"),
day == "Sun"
))
map +   stat_density2d(aes(x = long, y = lat,
fill = ..level..), geom = "polygon",
n = grid.points,
data = clean.topics %>% filter(user_language == all_langs$language[1],
source != "Instagram",
hour.cut == "12am-4am",
tz >= "2016-12-31",
tz <= "2017-01-01"
)) + theme(legend.position = "none")
LA.clean <- merge_langs(LA.all)
clean.topics <- merge_topics(LA.clean, "Topic_Data/")
top_langs <- clean.topics %>% group_by(lang.topic) %>%
count() %>%
mutate(percent = round(100*n/nrow(clean.topics),2)) %>%
arrange(desc(percent))
source("SpatialUtil.R")
meters <- LongLatToM(clean.topics$long,clean.topics$lat,epsg)
topic_meters <- cbind(clean.topics,meters)
LAX.map <- ggmap(get_map(location = bbox, source = "stamen", maptype="toner", color = "bw")) +
theme(axis.text.x = element_blank(),
axis.title.x = element_blank(),
axis.text.y = element_blank(),
axis.title.y = element_blank(),
axis.ticks = element_blank())
map <- LAX.map
grid.points <- findN(map,200,epsg)
map + scale_color_brewer(palette = "Set1", name ="Topics") +
geom_density2d(aes(x = long, y = lat, color = lang.topic), alpha = .5,
n = grid.points,
data = clean.topics %>% filter(source != "Instagram",
lang.topic %in% top_langs$lang.topic[1:6])) +
facet_grid(day ~ hour.cut) +
theme(legend.position = "bottom", legend.box = "horizontal",
legend.box.margin = margin(0,0,0,0),
plot.margin = margin(0,10,10,10)) +
ggsave(paste0("Outputs/",db,"-TopicDensity_DayHour_NEW.png"), width = 6.4808, height = 8.0727, units = "in")
source("LAX_meta.R")
LA.clean <- merge_langs(LA.all)
clean.topics <- merge_topics(LA.clean, "Topic_Data/")
top_langs <- clean.topics %>% group_by(lang.topic) %>%
count() %>%
mutate(percent = round(100*n/nrow(clean.topics),2)) %>%
arrange(desc(percent))
source("SpatialUtil.R")
meters <- LongLatToM(clean.topics$long,clean.topics$lat,epsg)
topic_meters <- cbind(clean.topics,meters)
LAX.map <- ggmap(get_map(location = bbox, source = "stamen", maptype="toner", color = "bw")) +
theme(axis.text.x = element_blank(),
axis.title.x = element_blank(),
axis.text.y = element_blank(),
axis.title.y = element_blank(),
axis.ticks = element_blank())
map <- LAX.map
grid.points <- findN(map,200,epsg)
map + scale_color_brewer(palette = "Set1", name ="Topics") +
geom_density2d(aes(x = long, y = lat, color = lang.topic), alpha = .5,
n = grid.points,
data = clean.topics %>% filter(source != "Instagram",
lang.topic %in% top_langs$lang.topic[1:6])) +
facet_grid(day ~ hour.cut) +
theme(legend.position = "bottom", legend.box = "horizontal",
legend.box.margin = margin(0,0,0,0),
plot.margin = margin(0,10,10,10)) +
ggsave(paste0("Outputs/",db,"-TopicDensity_DayHour_NEW.png"), width = 6.4808, height = 8.0727, units = "in")
